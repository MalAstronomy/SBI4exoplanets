{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from vit_pytorch.vit_original import ViT\n",
    "# import torch\n",
    "# from torchsummary import summary\n",
    "# from torch import nn\n",
    "\n",
    "#962 = 13 * 74  - div into 74 patches \n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = (1, 962),\n",
    "#     patch_size = (1, 13),\n",
    "#     num_classes = 1,\n",
    "#     channels = 1,\n",
    "#     dim = 16,\n",
    "#     depth = 3,\n",
    "#     heads = 16,\n",
    "#     mlp_dim = 512,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1\n",
    "# )\n",
    "\n",
    "# img = torch.randn(1, 1, 1,962)\n",
    "# preds = model(img) \n",
    "\n",
    "# summary(model, (1, 1, 962))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vit_pytorch.vit_original as ViT\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViT_modified(nn.Module):\n",
    "    def __init__(self,  image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 1, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "\n",
    "        image_height, image_width = ViT.pair(image_size)\n",
    "        patch_height, patch_width = ViT.pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            ViT.Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = ViT.Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "                nn.LayerNorm(dim+13),\n",
    "                nn.Linear(dim+13, num_classes)\n",
    "            ) \n",
    "\n",
    "    def forward(self, image):\n",
    "            \n",
    "        img_log = (torch.log(img[:,:,:,13:])-torch.log(torch.exp(torch.Tensor([-18.]))))/torch.log(torch.exp(torch.Tensor([6.])))\n",
    "        x = self.to_patch_embedding(img_log)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = ViT.repeat(self.cls_token, '() n d -> b n d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "        x = self.to_latent(x)\n",
    "        params = torch.squeeze(img[:,:,:,:13], dim=1)\n",
    "        params = torch.squeeze(params, dim = 1)\n",
    "        \n",
    "        x = torch.cat((params,x),dim=1)\n",
    "        \n",
    "        return self.mlp_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT_modified(\n",
    "    image_size = (1, 962),\n",
    "    patch_size = (1, 13),\n",
    "    num_classes = 1,\n",
    "    channels = 1,\n",
    "    dim = 16,\n",
    "    depth = 3,\n",
    "    heads = 16,\n",
    "    mlp_dim = 512,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 1, 1,962)\n",
    "preds = model(img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 73, 16]              --\n",
      "|    └─Rearrange: 2-1                    [-1, 73, 13]              --\n",
      "|    └─Linear: 2-2                       [-1, 73, 16]              224\n",
      "├─Dropout: 1-2                           [-1, 74, 16]              --\n",
      "├─Transformer: 1-3                       [-1, 74, 16]              --\n",
      "├─Identity: 1-4                          [-1, 16]                  --\n",
      "├─Sequential: 1-5                        [-1, 1]                   --\n",
      "|    └─LayerNorm: 2-3                    [-1, 29]                  58\n",
      "|    └─Linear: 2-4                       [-1, 1]                   30\n",
      "==========================================================================================\n",
      "Total params: 312\n",
      "Trainable params: 312\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.49\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 73, 16]              --\n",
       "|    └─Rearrange: 2-1                    [-1, 73, 13]              --\n",
       "|    └─Linear: 2-2                       [-1, 73, 16]              224\n",
       "├─Dropout: 1-2                           [-1, 74, 16]              --\n",
       "├─Transformer: 1-3                       [-1, 74, 16]              --\n",
       "├─Identity: 1-4                          [-1, 16]                  --\n",
       "├─Sequential: 1-5                        [-1, 1]                   --\n",
       "|    └─LayerNorm: 2-3                    [-1, 29]                  58\n",
       "|    └─Linear: 2-4                       [-1, 1]                   30\n",
       "==========================================================================================\n",
       "Total params: 312\n",
       "Trainable params: 312\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.49\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, (1,1,960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a52cde6f5da16726e1e0474ad78e23286cb43c3a3728cb0a197cc2311bc1810"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('petitRT': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
