{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "\n",
    "import os\n",
    "# os.environ[\"pRT_input_data_path\"] = \"/home/mvasist/pRT/input_data\"\n",
    "\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "import pymultinest\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "from petitRADTRANS import Radtrans\n",
    "from petitRADTRANS import nat_cst as nc \n",
    "from petitRADTRANS.retrieval.parameter import Parameter\n",
    "from petitRADTRANS.retrieval.models import emission_model_diseq\n",
    "\n",
    "from sbi.inference import SNRE_A, SNRE, prepare_for_sbi, simulate_for_sbi\n",
    "from sbi.utils.get_nn_models import posterior_nn\n",
    "from sbi import utils as utils\n",
    "from sbi.types import Array, OneOrMore, ScalarFloat\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import csv\n",
    "import h5py\n",
    "import json\n",
    "from pathlib import Path\n",
    "import math\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torchsummary import summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mvasist/scripts_new'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 1.9.1\n",
      "TorchVision: 0.10.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch: {torch.__version__}\")\n",
    "print(f\"TorchVision: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Wed_Oct_23_19:24:38_PDT_2019\n",
      "Cuda compilation tools, release 10.2, V10.2.89\n"
     ]
    }
   ],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_file = \"/home/mvasist/scripts_new/datasets/13params_traintest_vit.h5\"\n",
    "\n",
    "# with h5py.File(path_to_file, 'r') as hdf:\n",
    "#     data = np.array(hdf.get('data')).astype(np.float32)\n",
    "#     label = np.array(hdf.get('labels')).astype(np.float32)\n",
    "\n",
    "# hdf.close()\n",
    "\n",
    "# print(np.shape(data)) #[params,spectra]\n",
    "# data = np.pad(data, ((0,0),(0,0),(0,2)), 'constant')\n",
    "# print(np.shape(data))  #962\n",
    "# print(np.shape(data[:,:,13:])) #949\n",
    "\n",
    "# # Normalize\n",
    "# a_min = np.min(data[:,:,13:])  #axis=0\n",
    "# a_max = np.max(data[:,:,13:])\n",
    "# spec_norm = (data[:,:,13:]-a_min)/(a_max-a_min)\n",
    "# print(np.shape(spec_norm))\n",
    "# data_norm = np.c_[data[:,:,:13], spec_norm]\n",
    "# print(np.shape(data_norm))\n",
    "\n",
    "# #Tensor\n",
    "# target = torch.from_numpy(label)\n",
    "# inpt = torch.from_numpy(data_norm)\n",
    "# inpt_ = inpt.unsqueeze_(1)\n",
    "# inpt_ch = transforms.Lambda(lambda x: x.repeat(1, 3, 1, 1))(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for ig in glob.iglob('/home/mvasist/scripts_new/datasets/dataset/_/onehot/*.h5'):\n",
    "#     print(ig)\n",
    "\n",
    "files = glob.glob('/home/mvasist/scripts_new/datasets/dataset/_/onehot/*.h5')\n",
    "len(files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5Dataset(data.Dataset):\n",
    "    \"\"\"Represents an abstract HDF5 dataset.\n",
    "    \n",
    "    Input params:\n",
    "        file_path: Path to the folder containing the dataset (one or multiple HDF5 files).\n",
    "        load_data: If True, loads all the data immediately into RAM. Use this if\n",
    "            the dataset is fits into memory. Otherwise, leave this at false and \n",
    "            the data will load lazily.\n",
    "        data_cache_size: Number of HDF5 files that can be cached in the cache (default=3).\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, load_data, data_cache_size=2):\n",
    "        super().__init__()\n",
    "        self.data_info = []\n",
    "        self.data_cache = {}\n",
    "        self.data_cache_size = data_cache_size\n",
    "\n",
    "        # Search for all h5 files\n",
    "        p = Path(file_path)\n",
    "        #print(p)\n",
    "        assert(p.is_dir())\n",
    "        \n",
    "        files = sorted(p.glob('13params_vit__1_*.h5'))     ###################################################\n",
    "        if len(files) < 1:\n",
    "            raise RuntimeError('No hdf5 datasets found')\n",
    "\n",
    "        for h5dataset_fp in files:\n",
    "            self._add_data_infos(str(h5dataset_fp.resolve()), load_data)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        # get data\n",
    "        x = self.get_data(\"data\", index) #cache data\n",
    "        x = torch.from_numpy(x)\n",
    "        x = x.unsqueeze_(1)\n",
    "        #x = transforms.Lambda(lambda h: h.repeat(1, 3, 1, 1))(x) #adding 3 channels \n",
    "        x = torch.nn.functional.pad(x, (0, 2, 0, 0)) #padding - 962 div by 13\n",
    "        #x = x.view(-1,3,1,962)\n",
    "        #print('get_item: ', x.size())\n",
    "\n",
    "        # get label\n",
    "        y = self.get_data(\"label\", index) \n",
    "        y = torch.from_numpy(y)\n",
    "        #y = y.view(-1,1)\n",
    "        #print('len: ', y.size())\n",
    "        return (x, y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.get_data_infos('data'))\n",
    "    \n",
    "    def _add_data_infos(self, file_path, load_data):\n",
    "        with h5py.File(file_path, 'r') as h5_file:\n",
    "            # Walk through all datasets, extracting them\n",
    "            for dname, ds in h5_file.items():\n",
    "                #print(dname, ds)\n",
    "                # if data is not loaded its cache index is -1\n",
    "                idx = -1\n",
    "                if load_data:\n",
    "                    # add data to the data cache\n",
    "                    idx = self._add_to_cache(h5_file[dname][()], file_path)\n",
    "\n",
    "                # type is derived from the name of the dataset; we expect the dataset\n",
    "                # name to have a name such as 'data' or 'label' to identify its type\n",
    "                # we also store the shape of the data in case we need it\n",
    "                self.data_info.append({'file_path': file_path, 'type': dname, 'shape': h5_file[dname][()].shape, 'cache_idx': idx})\n",
    "\n",
    "    def _load_data(self, file_path): #into cache\n",
    "        \"\"\"Load data to the cache given the file\n",
    "        path and update the cache index in the\n",
    "        data_info structure.\n",
    "        \"\"\"\n",
    "        with h5py.File(file_path,'r') as h5_file:\n",
    "            for dname, ds in h5_file.items():\n",
    "                #print(dname)\n",
    "                idx = self._add_to_cache(h5_file[dname][()], file_path) #0 for data, 1 for labels\n",
    "\n",
    "                # find the beginning index of the hdf5 file we are looking for\n",
    "                file_idx = next(i for i,v in enumerate(self.data_info) if v['file_path'] == file_path)\n",
    "\n",
    "                # the data info should have the same index since we loaded it in the same way\n",
    "                self.data_info[file_idx + idx]['cache_idx'] = idx\n",
    "                \n",
    "\n",
    "        # remove an element from data cache if size was exceeded\n",
    "        if len(self.data_cache) > self.data_cache_size:\n",
    "            # remove one item from the cache at random\n",
    "            removal_keys = list(self.data_cache)\n",
    "            removal_keys.remove(file_path)\n",
    "            self.data_cache.pop(removal_keys[0])\n",
    "            # remove invalid cache_idx\n",
    "            self.data_info = [{'file_path': di['file_path'], 'type': di['type'], 'shape': di['shape'], 'cache_idx': -1} if di['file_path'] == removal_keys[0] else di for di in self.data_info]\n",
    "\n",
    "    def _add_to_cache(self, data, file_path):\n",
    "        \"\"\"Adds data to the cache and returns its index. There is one cache\n",
    "        list for every file_path, containing all datasets in that file.\n",
    "        \"\"\"\n",
    "        if file_path not in self.data_cache:\n",
    "            self.data_cache[file_path] = [data]\n",
    "        else:\n",
    "            self.data_cache[file_path].append(data)\n",
    "            \n",
    "        return len(self.data_cache[file_path]) - 1\n",
    "\n",
    "    def get_data_infos(self, type):\n",
    "        \"\"\"Get data infos belonging to a certain type of data.\n",
    "        \"\"\"\n",
    "        \n",
    "        data_info_type = [di for di in self.data_info if di['type'] == type]\n",
    "        return data_info_type\n",
    "        \n",
    "\n",
    "    def get_data(self, type, i):\n",
    "        # This method loads the data in the file that the spectrum is in\n",
    "        \n",
    "        \"\"\"Call this function anytime you want to access a chunk of data from the\n",
    "            dataset. This will make sure that the data is loaded in case it is\n",
    "            not part of the data cache.\n",
    "        \"\"\"\n",
    "        #size of dataset in each file\n",
    "        \n",
    "        fp = self.get_data_infos(type)[i]['file_path']\n",
    "        if fp not in self.data_cache:\n",
    "            self._load_data(fp)\n",
    "        \n",
    "        # get new cache_idx assigned by _load_data_info\n",
    "        cache_idx = self.get_data_infos(type)[i]['cache_idx']\n",
    "        return self.data_cache[fp][cache_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = HDF5Dataset('/home/mvasist/scripts_new/datasets/dataset/_/onehot/', load_data=False, data_cache_size=4)\n",
    "# print(len(dataset))\n",
    "\n",
    "split = [0.9, 0.1]\n",
    "split_train = '0.9'\n",
    "batch_size = 4 \n",
    "indices = list(range(len(dataset)))\n",
    "s = int(np.floor(split[1] * len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling\n",
    "np.random.seed(111)\n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[s:], indices[:s]\n",
    "#print(train_indices, val_indices)\n",
    "train_sampler, val_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=2, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=2, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1, 1, 962])\n",
      "torch.Size([200, 2])\n"
     ]
    }
   ],
   "source": [
    "img = dataset.__getitem__(2)\n",
    "print(np.shape(img[0]))\n",
    "print(np.shape(img[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(train_dataloader.dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 200, 2])\n"
     ]
    }
   ],
   "source": [
    "for ind, s in enumerate(train_dataloader):\n",
    "#     print(ind)\n",
    "#     print(s[0].size(), s[1].size())\n",
    "#     s_0 = s[0].view(-1,1,1,962)\n",
    "#     print(s_0.size())\n",
    "    print(np.shape(s[1]))\n",
    "    break\n",
    "    \n",
    "# print('')    \n",
    "\n",
    "# for ind, s in enumerate(val_dataloader):\n",
    "#     print(ind,np.shape(s[0]), np.shape(s[1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'], dataloaders['val'] = train_dataloader, val_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 49, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloaders['train']), len(dataloaders['train'].dataset), len(dataloaders['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    #print(y_pred.argmax(axis=1), y_test.argmax(axis=1))\n",
    "    correct_results_sum = (y_pred.argmax(axis=1) == y_test.argmax(axis=1)).sum().float()\n",
    "    #print('crs', y_pred.argmax(axis=1) == y_test.argmax(axis=1), correct_results_sum, len(y_test))\n",
    "    acc = correct_results_sum/len(y_test)\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "print(dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Sequential: 1-1                        [-1, 73, 16]              --\n",
      "|    └─Rearrange: 2-1                    [-1, 73, 13]              --\n",
      "|    └─Linear: 2-2                       [-1, 73, 16]              224\n",
      "├─Dropout: 1-2                           [-1, 74, 16]              --\n",
      "├─Transformer: 1-3                       [-1, 74, 16]              --\n",
      "├─Identity: 1-4                          [-1, 16]                  --\n",
      "├─Sequential: 1-5                        [-1, 1]                   --\n",
      "|    └─LayerNorm: 2-3                    [-1, 29]                  58\n",
      "|    └─Linear: 2-4                       [-1, 1]                   30\n",
      "==========================================================================================\n",
      "Total params: 312\n",
      "Trainable params: 312\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.49\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.01\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.01\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Sequential: 1-1                        [-1, 73, 16]              --\n",
       "|    └─Rearrange: 2-1                    [-1, 73, 13]              --\n",
       "|    └─Linear: 2-2                       [-1, 73, 16]              224\n",
       "├─Dropout: 1-2                           [-1, 74, 16]              --\n",
       "├─Transformer: 1-3                       [-1, 74, 16]              --\n",
       "├─Identity: 1-4                          [-1, 16]                  --\n",
       "├─Sequential: 1-5                        [-1, 1]                   --\n",
       "|    └─LayerNorm: 2-3                    [-1, 29]                  58\n",
       "|    └─Linear: 2-4                       [-1, 1]                   30\n",
       "==========================================================================================\n",
       "Total params: 312\n",
       "Trainable params: 312\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.49\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vit_pytorch.efficient import ViT\n",
    "from linformer import Linformer\n",
    "from vit_pytorch import ViT as ViT_modified\n",
    "\n",
    "#962 = 13 * 74  - div into 74 patches \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #torch.device(\"cpu\") #\n",
    "\n",
    "model = ViT_modified(n_classes = 1,\n",
    "                    image_size = (1, 962),  # image size is a tuple of (height, width)\n",
    "                    patch_size = (1, 13),    # patch size is a tuple of (height, width)\n",
    "                    dim = 16,\n",
    "                    depth = 3,\n",
    "                    heads = 16,\n",
    "                    mlp_dim = 512,\n",
    "                    dropout = 0.1,\n",
    "                    emb_dropout = 0.1\n",
    "                ).to(device)\n",
    "\n",
    "summary(model, (1, 1, 962))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#print(n.element_size() * n.nelement() /1e6, 'MB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "\n",
    "def train(n_epochs, model):\n",
    "    \n",
    "    best_loss = 0.0\n",
    "    for epoch in range(n_epochs):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                s = 0\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                s = 1\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                        \n",
    "            #both    \n",
    "            running_loss = 0.0\n",
    "            a = 0 \n",
    "            for batch_idx, sample in enumerate(dataloaders[phase]):\n",
    "                inputs = sample[0].view(-1,1,1,962).to(device)\n",
    "                target = sample[1][0].view(-1,1).long().to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    output = model(inputs) #[None, ...]\n",
    "                    loss = criterion(output, target) #torch.squeeze() .argmax(axis=1)\n",
    "                    #acc = binary_acc(torch.squeeze(output), torch.squeeze(target))\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()   \n",
    "                \n",
    "                    running_loss += 1 * loss.item() * inputs.size(0) #loss for the phase/whole dataset\n",
    "                \n",
    "                if batch_idx % 1 == 0: \n",
    "                    a+= len(sample[0])\n",
    "                    print('{} epoch: {} [{}/{} ({:0.0f}%)]\\tLoss: {:.6f}\\tAcc: {:.2f}'.format(phase,epoch,\\\n",
    "                            a,int(np.ceil(len(dataset)*split[s])),np.floor((100.*a)/(len(dataset)*split[s])), loss.item(), acc))\n",
    "                                \n",
    "            if phase == 'train':\n",
    "                metrics[phase+'_loss'].append(running_loss/int(dataset_size*split[0]))\n",
    "            else:\n",
    "                metrics[phase+'_loss'].append(running_loss/int(dataset_size*split[1]))\n",
    "\n",
    "            if phase == 'val': \n",
    "                if epoch ==  (n_epochs-1) or running_loss < best_loss:\n",
    "                    print('saving')\n",
    "                    best_loss = running_loss\n",
    "                    model_path = os.path.join(model_dir, 'model_vit.pth')\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    \n",
    "        with open(metrics_path, 'w') as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "                    \n",
    "#         print('--------------------------------------------------------------------')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 1\n",
    "criterion = nn.BCEWithLogitsLoss() #nn.BCELoss() nn.CrossEntropyLoss()\n",
    "# summary(model, (1, 1, 960))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=float(0.001))\n",
    "\n",
    "model_dir = '/home/mvasist/scripts_new/model/'\n",
    "metrics_path = os.path.join(model_dir, 'metrics__vit.json')\n",
    "\n",
    "metrics = {\n",
    "    'model': model_dir,\n",
    "    'optimizer': optimizer.__class__.__name__,\n",
    "    'criterion': criterion.__class__.__name__,\n",
    "#     'scheduler': scheduler.__class__.__name__,\n",
    "    'dataset_size': int(len(dataset)),\n",
    "    'train_size': int(split[0]*len(dataset)),\n",
    "    'test_size': int(split[1]*len(dataset)),\n",
    "    'n_epoch': nb_epoch,\n",
    "    'batch_size': batch_size,\n",
    "#     'learning_rate': [],\n",
    "    'train_loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "\n",
    "train(nb_epoch, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_dataset_size= 1000\n",
    "# Define name of the hdf5 file containing the test data:\n",
    "path_to_datasets = '/home/mvasist/scripts_new/model/datasets/'\n",
    "test_file = path_to_datasets+\"13params_traintest_bce_trial.h5\"\n",
    "\n",
    "transfo = transforms.Compose([Normalize(), ToTensor(), Resize(256), CenterCrop(224)])\n",
    "\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# Create the dataset object:xc\n",
    "test_dataset = spectrum_dataset(path_to_file = test_file, \n",
    "                          size = test_dataset_size,\n",
    "                          transform = transfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = []\n",
    "ratio_list = []\n",
    "p= []\n",
    "acc=0\n",
    "\n",
    "for id in range(test_dataset_size):\n",
    "    sample = test_dataset[id]\n",
    "    lteratio = sample['target'].numpy()\n",
    "    spectrum = sample['input'].unsqueeze(0)\n",
    "    prediction = model_loop(spectrum).detach().numpy()[0]\n",
    "\n",
    "    prediction_list.append(prediction)\n",
    "    ratio_list.append(lteratio)\n",
    "\n",
    "    if (prediction > 0.5): \n",
    "        p.append(np.float32([1]))\n",
    "    else: \n",
    "        p.append(np.float32([0]))\n",
    "\n",
    "    if p[id] == lteratio: \n",
    "        acc+=1\n",
    "\n",
    "acc = (acc/test_dataset_size)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc,id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = inference.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = torch.load('observations/6param_observation_TintLkIRLgLH2OLCH4LCO.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = time.time()\n",
    "sampls= 1000\n",
    "\n",
    "samples = posterior.sample((sampls,), x=observation)\n",
    "log_probability = posterior.log_prob(samples, x= observation)\n",
    "end= time.time()\n",
    "print('it takes: '+ str((end-start)/3600) + ' hrs')\n",
    "\n",
    "#save samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2014)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10\n",
    "output = torch.full([10, 64], 1.5)  # A prediction (logit)\n",
    "pos_weight = torch.ones([64])  # All weights are equal to 1\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a52cde6f5da16726e1e0474ad78e23286cb43c3a3728cb0a197cc2311bc1810"
  },
  "kernelspec": {
   "display_name": "Python [conda env:petitRT]",
   "language": "python",
   "name": "conda-env-petitRT-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
